{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load SQL table to DataFrame\n",
    "def load_sql(db_name, tbl_name):\n",
    "  \"\"\"Load SQLite database.\"\"\"\n",
    "  con = sqlite3.connect(f'database/{db_name}.db')\n",
    "  df = pd.read_sql(f\"SELECT * FROM {tbl_name}\", con)\n",
    "  con.close()\n",
    "  return df\n",
    "\n",
    "def timestamp():\n",
    "  \"\"\"Create current timestamp, e.g., 20221107_123045.\"\"\"\n",
    "  return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def save_csv(df, filename):\n",
    "  \"\"\"Save dataframe into CSV file.\"\"\"\n",
    "  filepath = Path(f'datasets/{filename}_{timestamp()}.csv')\n",
    "  filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "  df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter dataframe\n",
    "df = load_sql('tweets_v7', 'tweets_v7')\n",
    "df[['text']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset into CSV\n",
    "save_csv(df, \"tweets_v7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "counter = None\n",
    "\n",
    "def validate(token):\n",
    "  return len(token) > 1 and token not in ['...', \"n't\", 'it', 'do']\n",
    "\n",
    "for idx, tweet in df.iterrows():\n",
    "  _tokens = nltk.tokenize.casual_tokenize(tweet['text'])\n",
    "  tokens = []\n",
    "  \n",
    "  for token in _tokens:\n",
    "    if validate(token):\n",
    "      tokens.append(token)\n",
    "  \n",
    "  if counter is None:\n",
    "    counter = Counter(tokens)\n",
    "  else:\n",
    "    counter.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform counter into dataframe\n",
    "min_freq = 10\n",
    "freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "freq_df = freq_df.query('freq >= @min_freq')\n",
    "freq_df.index.name = 'token'\n",
    "\n",
    "freq_df.sort_values(by='freq', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency diagram\n",
    "ax = freq_df.sort_values(by='freq', ascending=False).head(30).plot(kind='barh', width=0.8, figsize=(10,7))\n",
    "ax.invert_yaxis()\n",
    "ax.set(xlabel='Frequency', ylabel='Token', title='Most Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = freq_df.sort_values(by='freq', ascending=False).tail(20).plot(kind='barh', width=0.9, figsize=(6,8))\n",
    "ax.invert_yaxis()\n",
    "ax.set(xlabel='Frequency', ylabel='Token', title='Least Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=100, stopwords=None):\n",
    "  wc = WordCloud(width=800, height=600,\n",
    "                 background_color=\"white\", colormap=\"Paired\",\n",
    "                 max_font_size=200, max_words=max_words)\n",
    "  # generate the word cloud\n",
    "  wc.generate_from_frequencies(word_freq)\n",
    "  plt.imshow(wc, interpolation='bilinear')\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(counter, max_words=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 30 least common words\n",
    "counter.most_common()[-30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=5, max_df=0.7)\n",
    "count_vectors = count_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "W_lda_matrix = lda_model.fit_transform(count_vectors)\n",
    "H_lda_matrix = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "  \n",
    "  for topic, word_vector in enumerate(model.components_):\n",
    "    total = word_vector.sum()\n",
    "    largest = word_vector.argsort()[::-1] # invert sort order\n",
    "    print(\"\\nTopic %02d\" % topic)\n",
    "    \n",
    "    for i in range(0, no_top_words):\n",
    "      print(\" %s (%2.2f)\" % (features[largest[i]], word_vector[largest[i]] * 100.0 / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_model, count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worldcloud_topic(model, features, no_top_words=40):\n",
    "  for topic, words in enumerate(model.components_):\n",
    "    size = {}\n",
    "    largest = words.argsort()[::-1] # invert sort order\n",
    "    \n",
    "    for i in range(0, no_top_words):\n",
    "      size[features[largest[i]]] = abs(words[largest[i]])\n",
    "      \n",
    "    wc = WordCloud(background_color='white', max_words=100, width=960, height=540)\n",
    "    wc.generate_from_frequencies(size)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'figures/{topic}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldcloud_topic(lda_model, count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Clustering to Uncover Structure of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "k_means_text = KMeans(n_clusters=10, random_state=42)\n",
    "k_means_text.fit(count_vectors)\n",
    "\n",
    "KMeans(n_clusters=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(k_means_text.labels_, return_counts=True)\n",
    "\n",
    "sizes = []\n",
    "\n",
    "for i in range(10):\n",
    "  sizes.append({\"cluster\": i, \"size\": np.sum(k_means_text.labels_ == i)})\n",
    "  \n",
    "pd.DataFrame(sizes).set_index(\"cluster\").sort_values(by='size', ascending=False).plot.bar(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_clusters(model, vectors, features, no_top_words=40):\n",
    "  for cluster in np.unique(model.labels_):\n",
    "    size = {}\n",
    "    words = vectors[model.labels_ == cluster].sum(axis=0).A[0]\n",
    "    largest = words.argsort()[::-1] # invert sort order\n",
    "    \n",
    "    for i in range(0, no_top_words):\n",
    "      size[features[largest[i]]] = abs(words[largest[i]])\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
    "    wc.generate_from_frequencies(size)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(f\"figures/cluster{cluster}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_clusters(k_means_text, count_vectors, count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Distribution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_data = []\n",
    "days = np.unique(df['created_at'])\n",
    "\n",
    "for day in days:\n",
    "  W_day = lda_model.transform(count_vectors[days == day])\n",
    "  days_data.append([day] + list(W_day.sum(axis=0) / W_day.sum() * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "voc = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic in lda_model.components_:\n",
    "  important = topic.argsort()\n",
    "  top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
    "  topic_names.append(\"Topic \" + top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days = pd.DataFrame(days_data, columns=['day'] + topic_names).set_index('day')\n",
    "df_days.plot.area(figsize=(16,6))\n",
    "\n",
    "plt.title('Topics Distribition over Time')\n",
    "plt.savefig(f\"figures/topics_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships Between Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationships between categorical variables\n",
    "import pandas as pd\n",
    "\n",
    "# Temporal rule-based sentiment\n",
    "pd.crosstab(df.rulebased_sent, df.created_at)\n",
    "\n",
    "# Rule-based sentiment by area, e.g., country, region, county, district\n",
    "pd.crosstab(df.rulebased_sent, df.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.nb_sent, df.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.svm_sent, df.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.dl_sent, df.country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def timeseries_plot(df, x='created_at', y='rulebased_sent', title='Rule-based Sentiment'):\n",
    "  positive_df = df[[x,y]]\n",
    "  positive_df = positive_df.loc[positive_df[y] == 1]\n",
    "  positive_df = positive_df[[x,y]].groupby(x).count().reset_index()\n",
    "  \n",
    "  negative_df = df[[x,y]]\n",
    "  negative_df = negative_df.loc[negative_df[y] == 0]\n",
    "  negative_df = negative_df[[x,y]].groupby(x).count().reset_index()\n",
    "  \n",
    "  plt.figure(figsize=(12,6))\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Sentiment')\n",
    "  plt.title(title)\n",
    "  plt.plot(positive_df[x], positive_df[y], color='green')\n",
    "  plt.plot(negative_df[x], negative_df[y], color='red')\n",
    "  \n",
    "  ax = plt.gca()\n",
    "  ax.set_xticklabels(labels=positive_df[x], rotation=90)\n",
    "  ax.legend(['Positive', 'Negative'])\n",
    "  \n",
    "  plt.savefig(f\"figures/timeseries_{x}_{y}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_analysis(df, sent=1, title='Temporal Analysis', country=None):\n",
    "  columns = ['rulebased_sent', 'nb_sent', 'svm_sent', 'dl_sent']\n",
    "  colours = ['#E3000E', '#92F22A', '#EE543A', '#2C82C9']\n",
    "\n",
    "  df['created_at'] = pd.to_datetime(df['created_at']).dt.date\n",
    "\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.xlabel('Date')\n",
    "  plt.ylabel('Sentiments')\n",
    "  plt.title(title)\n",
    "\n",
    "  for idx, column in enumerate(columns):\n",
    "    plot = df[['created_at', column, 'country']]\n",
    "    plot = plot.loc[plot[column] == sent]\n",
    "\n",
    "    if country:\n",
    "      plot = plot.loc[plot['country'] == country]\n",
    "\n",
    "    plot = plot.sort_values(by='created_at', ascending=True)\n",
    "    plot = plot[['created_at', column]].groupby('created_at').count().reset_index()\n",
    "    plt.plot(plot['created_at'], plot[column], color=colours[idx])\n",
    "\n",
    "  xlabels = df[['created_at']].sort_values(by='created_at', ascending=True)\n",
    "  xlabels = xlabels.groupby('created_at').count().reset_index()\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.set_xticks(xlabels['created_at'])\n",
    "  ax.set_xticklabels(labels=xlabels['created_at'], rotation=90)\n",
    "  ax.legend(['VADER', 'MNB', 'SVM', 'BERTweet'])\n",
    "\n",
    "  plt.savefig(f\"figures/timeseries_{'positive' if sent == 1 else 'negative'}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_analysis(df, sent=1, title='Positive Sentiment')\n",
    "timeseries_analysis(df, sent=0, title='Negative Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_analysis(df, sent=1, title='Positive Sentiment in England', country='England')\n",
    "timeseries_analysis(df, sent=0, title='Negative Sentiment in England', country='England')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-2 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "chi2_contingency(pd.crosstab(df.rulebased_sent, df.country))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check negation words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0bce5332d9b88654a019aea55f7051ce95b103c5f3fdd9866140591a6d9e8e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
