{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# Create secure context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# API configuration\n",
    "app_api_key = 'API_KEY'\n",
    "app_api_secret_key = 'API_SECRET'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(app_api_key, app_api_secret_key)\n",
    "api = tweepy.API(auth,\n",
    "                 wait_on_rate_limit=True,\n",
    "                 retry_count=5, # number of attemps\n",
    "                 retry_delay=180) # number of seconds to wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pandas configuration\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search tweets\n",
    "def search_tweets(query, limit=100):\n",
    "  \"\"\"Get tweets from Twitter API based on the query.\"\"\"\n",
    "  tweets = tweepy.Cursor(api.search_tweets,\n",
    "                         q=query,\n",
    "                         tweet_mode='extended',\n",
    "                         lang='en',\n",
    "                         count=100).items(limit)\n",
    "  retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "  df = pd.json_normalize(retrieved_tweets)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_30_day(query, limit=100):\n",
    "  \"\"\"Get tweets from the last 30 days.\"\"\"\n",
    "  tweets = tweepy.Cursor(api.search_30_day,\n",
    "                         label='SentimentAnalysis',\n",
    "                         query=query,\n",
    "                         maxResults=100).items(limit)\n",
    "  retrieved_tweets = [tweet._json for tweet in tweets]\n",
    "  df = pd.json_normalize(retrieved_tweets)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to create a current timestamp\n",
    "def timestamp():\n",
    "  \"\"\"Create current timestamp, e.g., 20221107_123045.\"\"\"\n",
    "  return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Function to save DataFrame to SQLite database\n",
    "def save_sql(df, filename, action=\"replace\"):\n",
    "  \"\"\"Save dataframe to SQLite. Available actions: replace, append.\"\"\"\n",
    "  db_name = Path(f'database/{filename}.db')\n",
    "  db_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "  con = sqlite3.connect(db_name)\n",
    "  df.to_sql(filename, con, index=False, if_exists=action)\n",
    "\n",
    "# Function to load SQL table to DataFrame\n",
    "def load_sql(db_name, tbl_name):\n",
    "  \"\"\"Load SQLite database.\"\"\"\n",
    "  con = sqlite3.connect(f'database/{db_name}.db')\n",
    "  df = pd.read_sql(f\"SELECT * FROM {tbl_name}\", con)\n",
    "  con.close()\n",
    "  return df\n",
    "\n",
    "# Function to save DataFrame into CSV file\n",
    "def save_csv(df, filename):\n",
    "  \"\"\"Save dataframe into CSV file.\"\"\"\n",
    "  filepath = Path(f'datasets/{filename}_{timestamp()}.csv')\n",
    "  filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "  df.to_csv(filepath)\n",
    "\n",
    "# Function to load DataFrame from CSV file\n",
    "def load_csv(filename):\n",
    "  \"\"\"Load dataframe from CSV file.\"\"\"\n",
    "  filepath = Path(f'datasets/{filename}.csv')\n",
    "  return pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns from the raw Twitter dataset\n",
    "def extract_columns(df, method='tweets_search'):\n",
    "  \"\"\"Extract columns from the raw Twitter dataset.\"\"\"\n",
    "  \n",
    "  if (method == 'tweets_search'):\n",
    "    # Extract only what we need\n",
    "    df = df[['id', 'created_at','full_text', 'user.location',\n",
    "           'place.name', 'place.full_name', 'place.id', 'entities.hashtags']]\n",
    "    \n",
    "    # Standardise column names\n",
    "    column_mapping = {'id': 'id', 'created_at': 'created_at',\n",
    "                      'full_text': 'text', 'user.location': 'user_location',\n",
    "                      'place.name': 'place_name', 'place.id': 'place_id',\n",
    "                      'place.full_name': 'place_full_name', 'entities.hashtags': 'hashtags'}\n",
    "    \n",
    "  else:\n",
    "    # Extract only what we need\n",
    "    df = df[['id', 'created_at', 'text', 'user.location',\n",
    "           'place.name', 'place.full_name', 'place.id', 'entities.hashtags']]\n",
    "    \n",
    "    # Standardise column names\n",
    "    column_mapping = {'id': 'id', 'created_at': 'created_at',\n",
    "                      'text': 'text', 'user.location': 'user_location',\n",
    "                      'place.name': 'place_name', 'place.id': 'place_id',\n",
    "                      'place.full_name': 'place_full_name', 'entities.hashtags': 'hashtags'}\n",
    "    \n",
    "  # Define remaining columns\n",
    "  columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "  \n",
    "  # Select and rename these columns\n",
    "  return df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_id = '6416b8512febefc9' # United Kingdom place_id\n",
    "\n",
    "queries = [\n",
    "  # Milary actions\n",
    "  f\"('ukraine' OR #ukraine OR @ukraine OR 'üá∫üá¶') AND (military OR war OR warfare OR unprovoked OR border OR escalation OR conflict OR invasion OR attack OR tension OR force OR battalion OR unprovoked OR invade OR power OR offensive) AND place:{place_id} -filter:retweets\",\n",
    "  # Military equipment\n",
    "  f\"('ukraine' OR #ukraine OR @ukraine OR 'üá∫üá¶') AND (weapon OR javelin OR tank OR aircraft OR armour OR munition OR arms OR jet OR lethal OR equipment OR fuel OR rocket OR stringer OR patriot OR helmet OR rifle OR goggle OR vest OR grenade OR gun OR nuclear OR missile) AND place:{place_id} -filter:retweets\",\n",
    "  # Financial aid\n",
    "  f\"('ukraine' OR #ukraine OR @ukraine OR 'üá∫üá¶') AND (financial OR money OR economy OR economic OR donation OR subsidy OR loan OR budget OR commit OR pledge OR dollars OR euros OR pounds OR billion OR million OR grant OR fund OR cost  OR finance OR bank OR investment OR donor) AND place:{place_id} -filter:retweets\",\n",
    "  # Meidical aid\n",
    "  f\"('ukraine' OR #ukraine OR @ukraine OR 'üá∫üá¶') AND (medical OR supply OR supplies OR food OR emergency OR humanitarian OR medicine OR wounded OR victim OR hospital OR 'red cross' OR hygiene OR healthcare OR zeolite OR oxygen OR patient OR shipment OR doctor OR nurse) AND place:{place_id} -filter:retweets\",\n",
    "  # Misc.\n",
    "  f\"('ukraine' OR #ukraine OR @ukraine OR 'üá∫üá¶') AND ('united kingdom' OR 'uk' OR 'üá¨üáß' OR england OR 'üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø' OR scotland OR 'üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø' OR wales OR 'üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø' OR 'nothern ireland' OR 'boris johnson' OR 'liz truss' OR 'ben wallace' OR parliament) AND place:{place_id} -filter:retweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_30_day_queries = [\n",
    "  # Military actions\n",
    "  f\"(military OR war OR warfare OR unprovoked OR border OR escalation OR conflict OR invasion OR attack OR tension OR force OR battalion OR invade OR power OR offensive) (ukraine OR #ukraine OR @ukraine OR üá∫üá¶) place_country:GB\",\n",
    "  # Military equipment\n",
    "  f'(ukraine OR #ukraine OR @ukraine OR üá∫üá¶) (weapon OR javelin OR tank OR aircraft OR armour OR munition OR arms OR jet OR lethal OR equipment OR fuel OR rocket OR stringer OR rifle OR goggle OR vest OR grenade OR gun OR nuclear OR missle) place_country:GB',\n",
    "  # Financial aid\n",
    "  f'(ukraine OR #ukraine OR @ukraine OR üá∫üá¶) (financial OR money OR economy OR economic OR donation OR subsidy OR loan OR budget OR commit OR billion OR million OR grant OR fund OR cost OR finance OR bank OR investment OR donor) place_country:GB',\n",
    "  # Medical aid\n",
    "  f'(ukraine OR #ukraine OR @ukraine OR üá∫üá¶) (medical OR supply OR supplies OR food OR emergency OR humanitarian OR medicine OR wounded OR victim OR hospital OR red cross OR hygiene OR healthcare OR zeolite OR oxygen OR shipment) place_country:GB',\n",
    "  # Misc.\n",
    "  f'(ukraine OR #ukraine OR @ukraine OR üá∫üá¶) (united kingdom OR uk OR üá¨üáß OR england OR üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø OR scotland OR üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø OR wales OR üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø OR nothern ireland OR boris johnson OR liz truss OR ben wallace OR parliament) place_country:GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities from tweets\n",
    "def extract_entities(entity_list):\n",
    "  \"\"\"Extract entities from entity list.\"\"\"\n",
    "  entities = list()\n",
    "  if len(entity_list) != 0:\n",
    "    for item in entity_list:\n",
    "      for key, value in item.items():\n",
    "        if key == 'text':\n",
    "          value = value.lower()\n",
    "          if value not in entities:\n",
    "            entities.append(value)\n",
    "  return \",\".join(str(x) for x in entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_30_days_tweets = None\n",
    "\n",
    "for query in search_30_day_queries:\n",
    "  results = search_30_day(query, 1000)\n",
    "  \n",
    "  if not results.empty:\n",
    "    results = extract_columns(results, method=\"search_30_days\")\n",
    "    \n",
    "    if last_30_days_tweets is None:\n",
    "      last_30_days_tweets = results\n",
    "    else:\n",
    "      last_30_days_tweets =pd.concat([last_30_days_tweets, results], ignore_index=True)\n",
    "      \n",
    "last_30_days_tweets['hashtags'] = last_30_days_tweets['hashtags'].apply(extract_entities)\n",
    "\n",
    "save_csv(last_30_days_tweets, \"raw_tweets_30_days\")\n",
    "\n",
    "print(f\"Total number of tweets retrieved: {last_30_days_tweets.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_30_days_tweets.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "  # Variable to store tweets\n",
    "  tweets = None\n",
    "\n",
    "  # Execute search_tweets method for each query\n",
    "  for query in queries:\n",
    "    results = search_tweets(query, 1000)\n",
    "    \n",
    "    # Check if results are not empty\n",
    "    if not results.empty:\n",
    "      results = extract_columns(results)\n",
    "      \n",
    "    # Check if tweets variable is None\n",
    "    if tweets is None:\n",
    "      tweets = results\n",
    "    else:\n",
    "      tweets = pd.concat([tweets, results], ignore_index=True)\n",
    "      \n",
    "  # Extract hashtags from DataFrame\n",
    "  tweets['hashtags'] = tweets['hashtags'].apply(extract_entities)\n",
    "  \n",
    "  # Save raw tweets\n",
    "  save_csv(tweets, \"raw_tweets\")\n",
    "  \n",
    "  # Show number of tweets\n",
    "  print(f\"Total number of tweets retrieved: {tweets.shape[0]}\")\n",
    "  \n",
    "  # Return twets\n",
    "  return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "##### Check impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Identify noise\n",
    "def impurity(text, min_len=10):\n",
    "  \"\"\"Returns a share of suspicious characters in a text.\"\"\"\n",
    "  RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "  if text == None or len(text) < min_len:\n",
    "    return 0\n",
    "  else:\n",
    "    return len(RE_SUSPICIOUS.findall(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalise text with RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "# Remove noise with regular expressions\n",
    "def regex_normalise_text(text):\n",
    "  \"\"\"Remove noise from text with regular expressions.\"\"\"\n",
    "  \n",
    "  # convert html escapes like &amp; to characters\n",
    "  text = html.unescape(text)\n",
    "  \n",
    "  # tags like <tab>\n",
    "  text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "  \n",
    "  # markdown URLs like [Some text](https://....)\n",
    "  text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "  \n",
    "  # text or code in brackets like [0]\n",
    "  text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "  \n",
    "  # standalone sequences of specials, matches &# but not #cool\n",
    "  text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "  \n",
    "  # standalone sequences of hyphens like --- or ==\n",
    "  text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "  \n",
    "  # sequences of white spaces\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  # return clened text\n",
    "  return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add spaces between emoji icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Add spaces between emojis\n",
    "def add_space_between_emojis(text):\n",
    "  text = emoji.demojize(text)\n",
    "  emojis = re.findall(r\":+[a-zA-Z0-9_]+:\", text)\n",
    "  for e in emojis:\n",
    "    text = text.replace(e, ' ' + e + ' ')\n",
    "  text.replace('  ', ' ')\n",
    "  return emoji.emojize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normaliase text with Textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.preprocessing as tprep\n",
    "\n",
    "def normalise_text(text):\n",
    "  \"\"\"Normalise text with Textacy.\"\"\"\n",
    "  \n",
    "  # text = tprep.replace.currency_symbols(text)\n",
    "  text = tprep.replace.urls(text)\n",
    "  text = tprep.replace.hashtags(text)\n",
    "  text = tprep.replace.user_handles(text)\n",
    "  # text = tprep.replace.numbers(text)\n",
    "  \n",
    "  text = tprep.normalize.hyphenated_words(text)\n",
    "  text = tprep.normalize.quotation_marks(text)\n",
    "  text = tprep.normalize.unicode(text)\n",
    "  text = tprep.normalize.whitespace(text)\n",
    "  text = tprep.normalize.repeating_chars(text, chars='?')\n",
    "  text = tprep.normalize.repeating_chars(text, chars='!')\n",
    "  text = tprep.normalize.repeating_chars(text, chars='.')\n",
    "  \n",
    "  text = tprep.remove.punctuation(text, only='.')\n",
    "  text = tprep.remove.punctuation(text, only=',')\n",
    "  text = tprep.remove.punctuation(text, only=':')\n",
    "  text = tprep.remove.punctuation(text, only=';')\n",
    "  text = tprep.remove.brackets(text)\n",
    "  text = tprep.remove.html_tags(text)\n",
    "  text = tprep.remove.accents(text)\n",
    "  \n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change flag emoji to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "# Change flag emoji to text\n",
    "def flag_to_text(text):\n",
    "  \"\"\"Change flag emoji to text.\"\"\"\n",
    "  emoji_dict = demoji.findall(text)\n",
    "  flags_dict = {}\n",
    "  for emoji, name in emoji_dict.items():\n",
    "    if 'flag:' in name:\n",
    "      flags_dict[emoji] = name.replace('flag:', '')\n",
    "  for flag, name in flags_dict.items():\n",
    "    if flag in text:\n",
    "      text = text.replace(flag, name)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace characters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_clean_text(text):\n",
    "  \"\"\"Custom replacement of tokens within text.\"\"\"\n",
    "  trash = ('‚Äì', '+', '-', '/', '_URL_', '_USER_', '_TAG_', '_NUMBER_', '\"', '*', '‚Äî', '_', '‚Ä¢')\n",
    "  for elem in trash:\n",
    "    text = text.replace(elem, ' ')\n",
    "  return re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add spaces between potential words\n",
    "Don't use it, as it adds spaces between abbreviations and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_between_potential_words(text):\n",
    "  \"\"\"Python3 code to demonstarte working of add space between potential words suing regex() + list comprehension.\"\"\"\n",
    "  # printing original list\n",
    "  text = text.split(' ')\n",
    "  # using regex() to perform task\n",
    "  res = [re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", ele) for ele in text]\n",
    "  # printing result\n",
    "  return ' '.join(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case folding text (to lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from country_list import countries_for_language\n",
    "list_of_countries = dict(countries_for_language('en'))\n",
    "\n",
    "def casefolding(text):\n",
    "  \"\"\"Transform specific words into lowercase.\"\"\"\n",
    "  \n",
    "  acronyms = ['UK', 'US', 'EU', 'NATO']\n",
    "  countries = [list_of_countries[country] for country in list_of_countries]\n",
    "  cities = []\n",
    "  names = []\n",
    "  \n",
    "  for word in text.split(' '):\n",
    "    if word not in acronyms:\n",
    "      text = text.replace(word, word.lower())\n",
    "  \n",
    "  # for word in text.split(' '):\n",
    "  #   for match in re.findall(r\"([A-Z])\\w+/g\", text):\n",
    "  #     if match not in word and match not in acronyms:\n",
    "  #       text = text.replace(match, word.lower())\n",
    "        \n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove non-English words\n",
    "Don't use it, buggy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "  \"\"\"Remove non-English words with NLTK.\"\"\"\n",
    "  words = set(nltk.corpus.words.words())\n",
    "  return ' '.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardise country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_country_names(text):\n",
    "  \"\"\"Standardise country names, e.g., russias --> russia, united kingdom --> uk\"\"\"\n",
    "  # text = text.replace('russian', 'russia')\n",
    "  # text = text.replace('ukrainian', 'ukraine')\n",
    "  text = text.replace('russias', 'russia')\n",
    "  text = text.replace('RussoUkraine', 'Russia Ukraine')\n",
    "  text = text.replace('russoukraine', 'russia ukraine')\n",
    "  text = text.replace('ukraines', 'ukraine')\n",
    "  text = text.replace('united kingdom', 'uk')\n",
    "  text = text.replace('U K', 'uk')\n",
    "  text = text.replace('united states', 'us')\n",
    "  text = text.replace('USA', 'us')\n",
    "  return text.replace('usa', 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalise dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "  \"\"\"Format data to YYYY-MM-DD.\"\"\"\n",
    "  try:\n",
    "    # Fri Nov 04 16:26:06 +0000 2022\n",
    "    date = datetime.strptime(date, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "  except:\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add spaces between words where they potentially should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_between_numbers_and_text(text):\n",
    "  \"\"\"Add space between numbers and text, e.g., 100million --> 100 million\"\"\"\n",
    "  return re.sub('(\\d+(\\.\\d+)?)', r' \\1 ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_between_apostrophe_and_words(text):\n",
    "  \"\"\"Add spaces between apostrophe and word, e.g., then'yes' --> then 'yes', then'okay' --> then 'okay'\"\"\"\n",
    "  for word in text.split(' '):\n",
    "    for match in re.findall(r\"([A-Za-z]{2,}'[A-Za-z]{3,})\", text):\n",
    "      if match in word:\n",
    "        text = text.replace(match, match.replace(\"'\", \" '\"))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_between_punctuation_and_words(text, rpl='!?.,;:', steps=3):\n",
    "  \"\"\"Add spaces between punctuation and right adjacent word, e.g., Hello World!How are you? --> Hello World! How are you?\"\"\"\n",
    "  for _ in range(0, steps):\n",
    "    for word in text.split(' '):\n",
    "      for match in re.findall(r\"([A-Za-z]+[\" + rpl + \"][A-Za-z]+)\", text):\n",
    "        if match in word:\n",
    "          for r in rpl:\n",
    "            text = text.replace(match, match.replace(r, f\"{r} \"))\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group all cleaning tasks into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tweets):\n",
    "  # Copy new instance of tweets DataFrame\n",
    "  tweets_df = tweets.copy()\n",
    "  # Data cleaning tasks\n",
    "  tweets_df['text'] = tweets_df['text'].apply(add_space_between_emojis)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(regex_normalise_text)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(normalise_text)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(custom_clean_text)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(standardise_country_names)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(add_space_between_numbers_and_text)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(add_space_between_apostrophe_and_words)\n",
    "  tweets_df['text'] = tweets_df['text'].apply(add_space_between_punctuation_and_words)\n",
    "  # tweets_df['text'] = tweets_df['text'].apply(flag_to_text)\n",
    "  # Check the impurity of data\n",
    "  tweets_df['impurity'] = tweets_df['text'].apply(impurity)\n",
    "  # Additional cleaning of other columns\n",
    "  tweets_df['created_at'] = tweets_df['created_at'].apply(format_date)\n",
    "  \n",
    "  # Return clean DataFrame\n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection and Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = get_tweets()\n",
    "tweets = clean_data(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing tweets table\n",
    "existing_tweets_df = load_sql(\"tweets_v2\", \"tweets_v2\")\n",
    "# existing_tweets_df[['text']].sample(3)\n",
    "print(f\"Number of existing tweets: {existing_tweets_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new tweets with existing tweets\n",
    "new_tweets_df = pd.concat([existing_tweets_df, tweets]).drop_duplicates()\n",
    "print(f\"Total number of tweets: {new_tweets_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to SQLite database\n",
    "save_sql(new_tweets_df, \"tweets_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = load_sql(\"tweets_v2\", \"tweets_v2\")\n",
    "print(f\"Total number of tweets: {tweets_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lookup Geo Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def get_location(geo_id:str) -> dict:\n",
    "  \"\"\"Get the location details based on the geo_id returned from Twitter API.\"\"\"\n",
    "  resp = api.geo_id(geo_id)\n",
    "  \n",
    "  lon = resp.centroid[0]\n",
    "  lat = resp.centroid[1]\n",
    "  \n",
    "  resp = requests.get(f\"https://api.postcodes.io/postcodes?lon={lon}&lat={lat}&limit=1\")\n",
    "  resp = json.loads(resp.text)\n",
    "  \n",
    "  try:\n",
    "    resp = resp['result'][0]\n",
    "  \n",
    "    dict = {\n",
    "      'geo_id': geo_id,\n",
    "      'postcode': resp['postcode'],\n",
    "      'country': resp['country'],\n",
    "      'longitude': resp['longitude'],\n",
    "      'latitude': resp['latitude'],\n",
    "      'region': resp['region'],\n",
    "      'district': resp['admin_district'],\n",
    "      'county': resp['admin_county']\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(dict, orient='index').T\n",
    "  \n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.loc[tweets_df['place_id'] == '457b4814b4240d87']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.DataFrame()\n",
    "\n",
    "for key, tweet in tweets_df[579:].iterrows():\n",
    "  location = get_location(tweet['place_id'])\n",
    "  locations = locations.append(location, ignore_index=True)\n",
    "  print(location)\n",
    "  \n",
    "tweets_df = tweets_df.merge(locations, left_on='place_id', right_on='geo_id')\n",
    "tweets_df.drop('geo_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locations to database\n",
    "save_csv(locations, \"locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = load_csv('locations_20221111_125500')\n",
    "print(locations.shape)\n",
    "locations.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2 = tweets_df\n",
    "print(tweets_df2.shape)\n",
    "\n",
    "tweets_df2 = tweets_df.reset_index()\n",
    "locations = locations.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3 = tweets_df2.merge(locations, how='inner', left_on='place_id', right_on='geo_id')\n",
    "tweets_df3 = tweets_df3.drop_duplicates(subset=['text'])\n",
    "tweets_df3.drop(['index_x', 'index_y', 'id_y', 'level_0', 'geo_id', 'impurity'], axis=1, inplace=True)\n",
    "\n",
    "print(tweets_df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tweets_df[['place_id']]\n",
    "t2 = tweets_df3[['place_id']]\n",
    "print(t1.shape, t2.shape)\n",
    "\n",
    "t3 = pd.concat([t1,t2]).drop_duplicates(keep=False)\n",
    "print(t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.DataFrame()\n",
    "\n",
    "for key, tweet in t3.iterrows():\n",
    "  location = get_location(tweet['place_id'])\n",
    "  locations = locations.append(location, ignore_index=True)\n",
    "  print(location)\n",
    "  \n",
    "tweets_df4 = tweets_df.merge(locations, left_on='place_id', right_on='geo_id')\n",
    "tweets_df4.drop('geo_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(locations, 'locations2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = load_csv('locations_20221111_125500')\n",
    "locations2 = load_csv('locations2_20221117_220719')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations3 = pd.concat([locations, locations2])\n",
    "locations3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3 = tweets_df.merge(locations3, how='inner', left_on='place_id', right_on='geo_id')\n",
    "tweets_df3 = tweets_df3.drop_duplicates(subset=['text'])\n",
    "tweets_df3.drop(['id_y', 'geo_id', 'impurity'], axis=1, inplace=True)\n",
    "\n",
    "print(tweets_df3.shape)\n",
    "tweets_df3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sql(tweets_df3, 'tweets_v3', 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3 = load_sql('tweets_v3', 'tweets_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casefolding(\"The war in Ukraine is getting on nerves in some EU countries. Volodymir is pushing for support. UK is not better. What about Papua New Guinea, do they even care?\")\n",
    "\n",
    "tweets_df3['text'] = tweets_df3['text'].apply(casefolding)\n",
    "tweets_df3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "  return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sw_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3['text'] = tweets_df3['text'].apply(remove_stop_words)\n",
    "tweets_df3.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "  word_list = word_tokenize(text)\n",
    "  words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "  return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3['text'] = tweets_df3['text'].apply(lemmatize_words)\n",
    "tweets_df3[['text']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sql(tweets_df3, 'tweets_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3 = load_sql('tweets_v5', 'tweets_v5')\n",
    "save_csv(tweets_df3, 'tweets_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def unigram_tokens(text):\n",
    "  tokens = word_tokenize(text)\n",
    "  tokens = [t for t in tokens]\n",
    "  return ' | '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3['text_unigrams'] = tweets_df3['text'].apply(unigram_tokens)\n",
    "tweets_df3[['text_unigrams']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.extract import token_matches\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Complete function for noun phrase extraction based on PoS patterns\n",
    "def extract_noun_phrases(text):\n",
    "  doc = nlp(text)\n",
    "  patterns = []\n",
    "  preceding_pos = ['NOUN']\n",
    "  \n",
    "  for pos in preceding_pos:\n",
    "    # patterns.append([{\"POS\": pos}, {\"POS\": \"NOUN\", \"OP\": \"+\"}])\n",
    "    patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "    \n",
    "  spans = token_matches(doc, patterns=patterns)\n",
    "  return ['_'.join([t.lemma_ for t in s]) for s in spans]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3['text_bigrams'] = tweets_df3['text'].apply(extract_noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df3[['text', 'text_bigrams']].sample(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0bce5332d9b88654a019aea55f7051ce95b103c5f3fdd9866140591a6d9e8e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
